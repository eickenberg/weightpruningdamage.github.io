<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><script>
window.addEventListener('WebComponentsReady', function() {
  console.warn('WebComponentsReady');
  const loaderTag = document.createElement('script');
  loaderTag.src = 'https://distill.pub/template.v2.js';
  document.head.insertBefore(loaderTag, document.head.firstChild);
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-loader.js"></script>  


<html>
<head>
  <title>PIE: Pruning Identified Exemplars</title>
  <meta property="og:type" content="article"/>
  <meta property="og:title" content="PIE: Pruning Identified Exemplars"/>
  <meta property="og:description" content="Measuring the Disparate Impact of Model Pruning">
  <meta property="og:url" content="http://pair-code.github.io/saliency/"/>
  <meta property="og:image" content="http://pair-code.github.io/saliency/images/preview.png"/>
  <meta property="og:locale" content="en_US">
  <meta property="og:site_name" content="Google Research">
  <meta name="twitter:card" value="summary_large_image">
  <meta name="twitter:title" content="PIE: Pruning Identified Exemplars">
  <meta name="twitter:description" content="Measuring the disparate impact of model pruning on classes and individual images.">
  <meta name="twitter:url" content="http://pair-code.github.io/saliency/">
  <meta name="twitter:image" content="http://pair-code.github.io/saliency/images/preview.png">
  <meta name="twitter:image:width" content="560">
  <meta name="twitter:image:height" content="295">
  
  <!--  https://schema.org/Article -->
  <meta property="description" itemprop="description" content="Measuring the disparate impact of model pruning on classes and individual images.">

  <meta property="article:author" content="Sara Hooker">
  <meta property="article:author" content="Yann Dauphine">
  <meta property="article:author" content="Aaron Courville">
  <meta property="article:author" content="Andrea Frome">

  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
  <style>
    body {
      font-family: "Roboto", "Helvetica", sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      font-size: 12px;
    }

    html {
      margin: 0;
      padding: 0;
      height: 100%;
    }

    .grads-container {
      width: 64%;
    }

    .grads-container img {
      width: 100%;
      background: url('assets/loader.gif') no-repeat;
      background-position: center;
    }

    table td {
      font-size: 12px;
      text-align: center;
      outline: 1px solid white;
      padding: 0;
      margin: 0;
    }

    table.inner td {
      padding: 0;
      margin: 0;
      border: 0;
      width: 25%;
    }

    .footer-row {
      height: 15px;
    }

    .incorrect-row {
      background-color: #ffe8e8;
    }

    .incorrect-row td {
      outline: 1px solid #ffe8e8;
    }

    table.inner tr {
      border: 0;
    }

    table.inner th {
      padding: 8px;
    }

    table th {
      font-size: 11px;
    }

    table {
      border-collapse: collapse;
      border-spacing: 0;
    }

    thead, tbody { display: block; }

    .rotated {
      transform: rotate(90deg);
      transform-origin: left bottom 0;
      margin-top: -111px;
      font-weight: bold;
      font-size: 1.2em;
      padding: 8px;
    }

    #headers {
      z-index: 1000;
      background-color: white;
      height: 65px;
      vertical-align: middle;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }

    #headers span {
      background-color: white;
      display: inline-block;
      line-height: 65px;
      font-size: 1.2em;
      font-weight: bold;
      text-align: center;
      text-overflow: ellipsis;
      white-space: nowrap;
    }

    .cover {
      background: #1e283a;
    }

    .cover-container {
      padding-top: 10px;
      padding-bottom: 60px;
    }
    .descriptions {
      padding-top: 20px;
    }
    .cover-container, .descriptions, .attribution-container {
      padding-right: 15px;
      padding-left: 15px;
      margin-right: auto;
      margin-left: auto;
    }

    @media (min-width: 415px) {
      .cover-container, .descriptions, .attribution-container {
        width: 500px;
      }
    }

    @media (min-width: 768px) {
      .cover-container, .descriptions, .attribution-container {
        width: 650px;
      }
    }

    @media (min-width: 992px) {
      .cover-container, .descriptions, .attribution-container {
        width: 770px;
      }
    }

    @media (min-width: 1200px) {
      .cover-container, .descriptions, .attribution-container {
        width: 970px;
      }
    }
    
@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}
    

d-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}

    .cover h1 {
      font-family: "Roboto", "Gotham A", "Gotham B";
      letter-spacing: 0.05em;
      font-size: 63px;
      font-weight: 700;
      margin-bottom: 0.5em;
      text-transform: uppercase;
    }

    .cover h3 {
      font-size: 25px;
      letter-spacing: 0.05em;
      font-weight: 500;
    }

    .descriptions h3 {
      color: #313b4e;
      opacity: .8;
    }

    .cover {
      color: #ddd;
    }

    .authors {
      margin-top: -40px;
      font-size: 20px;
    }
    .institutions {
      margin-top: 10px;
      font-size: 15px;
    }
    .institutions a {
      font-size: 15px;
      color: #ccc;
    }

    .team-link {
      margin-top: 10px;
    }
    .team-link a {
      font-size: 15px;
      color: #ccc;
    }

    .subtitle {
      margin-top: -20px;
    }

    .icons {
      margin-top: 30px;
      padding-left: 4px;
    }

    .icons a {
      display: inline-block;
      font-size: 16px;
      color: #ccc;
      text-decoration: none;
    }

    .paper-icon {
      display: inline-block;
    }

    .paper-icon a {
      line-height: 35px;
      vertical-align: top;
    }

    .paper-icon:hover a {
      cursor: pointer;
      text-decoration: underline;
    }

    .description p {
      width: 75%;
      font-size: 16px;
    }

    .description img {
      vertical-align: middle;
      width: 100%;
    }

    .imgs-container {
      display: table-row;
    }

    .img-container {
      color: #62779c;
      text-align: center;
      font-weight: bold;
      font-size: 14px;
      padding-right: 8px;
      display: table-cell;
      width: 8%;
      height: 100%;
    }

    #headers.fixed-header {
      position: fixed;
      top: 0;
    }

    #table-container.fixed-header {
      margin-top: 106px;
    }

    .smoothgrad-label {
      color: #ff4e4e;
    }

    .image-label {
      font-size: 15px;
      text-align: left;
      padding-bottom: 4px;
      padding-top: 6px;
      padding-left: 2px;
      font-weight: normal;
    }

    .img-times-selector-container {
      margin-left: -80px;
      margin-top: -45px;
      font-size: 18px;
      font-weight: bold;
      text-align: center;
     }

    .img-times-selector {
      width: 175px;
    }

    .img-times-selector:hover {
      cursor: pointer;
    }

    .img-times-selector div {
      font-size: 12px !important;
      display: inline-block;
      padding: 4px;
    }

    .select-grad {
      border-top-left-radius: 5px;
      border-bottom-left-radius: 5px;
    }

    .select-img-times-grad {
      border-top-right-radius: 5px;
      border-bottom-right-radius: 5px;
      margin-left: -5px;
    }

    .grad-selected {
      background-color: #666;
      color: #eee;
    }
    .grad-unselected {
      background-color: #eee;
      color: #333;
    }
    .grad:hover {
      cursor: pointer;
      outline: 1px solid red !important;
    }
    #table {
      margin-top: 4px;
      width: 100%;
    }
    .grad-wrt-container {
      font-size: 14px;
      float: left;
    }
    .pointer-container {
      float: right;
      margin-bottom: 10px;
    }
    .main-img:hover {
      cursor: pointer;
    }

    .selected-gradient-wrt {
      cursor: auto !important;
      font-weight: bold;
      text-decoration: none !important;
    }
    .gradient-wrt {
      color: black;
      cursor: pointer;
      text-decoration: underline;
    }

    .show-mispred tr.correct-row {
      display: none;
    }

    .show-mispred tr.incorrect-row {
      display: table-row !important;
    }
    
.example-vis {
  display: flex;
  flex-wrap: wrap;
  flex-flow: row;
  /*align-items: center;*/
  justify-content: space-between;
  align-items: flex-start;
  min-height: 240px;
}
.example-vis > .chunk {
  /*width: calc(3 * 64px + 3 * 2px);*/
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  flex-flow: column;
  flex-wrap: wrap;
  align-items: flex-start;
  /*--padding-right: 46px;*/
  /*padding-right: 24px;*/
}
.example-vis > .chunk:last-child {
  padding-right: 0px;
}
.example-vis > .chunk > img {
  width: 64px;
  height: 64px;
  padding-right: 1px;
  padding-bottom: 1px;
  display: block;
}
.example-vis > .chunk > img:first {
  padding-bottom: 10px;
}
.example-vis > .chunk > .big {
  width: 147px;
  height: 147px;
}
.example-vis > .chunk > figcaption {
  margin-top: 15px;
}

.example-vis2 {
  display: -ms-flexbox;
  display: -webkit-flex;
  display: flex;
  flex-wrap: wrap;
  /*align-items: center;*/
  justify-content: flex-start;
  align-items: flex-start;
}

.example-vis2 > .chunk > img {
  width: 170px;
  height: 170px;
  padding-right: 10px;
}
.example-vis2 > .chunk > figcaption {
  margin-top: 5px;
}

.draft {
  border-left: 2px solid orange;
  padding-left: 5px;
  /*font-family: "Comic Sans MS", fantasy;*/
  color: orange;
}

d-slider {
  width: 100%;
}

d-title figure {
  margin: 0;
}

.shaded-figure {
  background-color: hsl(0, 0%, 97%);
  border-top: 1px solid hsla(0, 0%, 0%, 0.1);
  border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
  padding: 30px 0;
}

code,
tt {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 90%;
  background-color: hsl(0, 0%, 96%);
  border-radius: 3px;
}

figcaption code,
figcaption tt {
  color: rgba(0, 0, 0, 0.7);
}

code::before, 
code::after,
tt::before, 
tt::after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

#fv-vs-attribution {
  grid-auto-rows: 1fr; 
  grid-row-gap: 30px;
  grid-column-gap: 30px;
  margin: 1em 0 1.5em 0;
}

#fv-vs-attribution .images {
  display: grid; 
  grid-auto-columns: 1fr; 
  grid-auto-flow: column; 
  grid-column-gap: 5px;
}

#fv-vs-attribution figcaption {
   padding-top: 0.5em;
}

@media(min-width: 512px) {
  #fv-vs-attribution {
    grid-auto-columns: 1fr; 
    grid-auto-flow: column; 
    margin: 1em 0 1.5em 0;
  }
  
  #fv-vs-attribution .images {
    grid-column-gap: 10px;
  }
}
    
/* <d-front-matter>
  <script type="text/json">{
  "title": "Selective Brain Damage",
  "description": "Measuring the Disparate Impact of Model Compression",
  "authors": [
    {
      "author":"Sara Hooker",
      "authorURL":"https://colah.github.io/",
      "affiliation":"Google Brain Team",
      "affiliationURL":"https://g.co/brain"
    },
    {
      "author":"",
      "authorURL":"https://znah.net/",
      "affiliation":"Google Research",
      "affiliationURL":"https://research.google.com/"
    },
    {
      "author":"",
      "authorURL":"https://schubert.io/",
      "affiliation":"Google Brain Team",
      "affiliationURL":"https://g.co/brain"
    }
     {
      "author":"",
      "authorURL":"https://schubert.io/",
      "affiliation":"Google Brain Team",
      "affiliationURL":"https://g.co/brain"
    }
  ]
  }</script>
</d-front-matter> */
    
  </style>
</head>
<body>
  <div id="scroll-container">
    <div class="cover">
      <div class="cover-container">
        <div class="icons">
          <div class="paper-icon">
            <a href="https://arxiv.org/abs/1706.03825">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fpaper_icon.png?v=1572561063939" style="width: 100px"/><br>Paper
            </a>
          </div>
          <div class="paper-icon" style="margin-left: 20px">
            <a href="https://github.com/google-research/google-research/tree/master/pruning_identified_exemplars">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fcode_icon.png?v=1572562103868" style="width: 100px"/><br>Code
            </a>
          </div>    
        </div>
        <div class="title"><h2>Selective Brain Damage: Measuring the Disparate Impact of Model Compression</h2></div>
        </div>
        <div class="authors"></div>
      <div class="institutions"></div>
       </div>
    </div>
<d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>
      
        <p class="author">
          
            <a class="name" href="https://www.sarahooker.me/">Sara Hooker</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://g.co/brain">Google Brain, MILA</a>
        </p>
      
        <p class="author">
          
            <a class="name" href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&hl=en">Aaron Courville</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://g.co/brain">MILA</a>
        </p>
      
        <p class="author">
          
            <a class="name" href="http://www.dauphin.io/">Yann Dauphine</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://research.google.com/">Google Brain</a>
        </p>
      
      <p class="author">
          
            <a class="name" href="https://scholar.google.com/citations?user=f9_wq_kAAAAJ&hl=en"> Andrea Frome</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
        </p>
      
    </div>
    <div>
      <h3>Published</h3>
      
        <p>Nov. 7, 2019</p> 
  </div>
     </div>
</d-byline>
    <div class="descriptions">
       <p> add note here saying this template was made available by James Wexler <p> 
      <h3>What is lost when we prune deep neural networks?</h3>
      <div class="description">
          <p>Between infancy and adulthood, the number of synapses in our brain first multiply and then fall. 
            Synaptic pruning improves efficiency by removing redundant neurons and strengthening synaptic connections 
            that are most useful for the environment.
            Despite losing 50 % of all synapses between age two and ten, the brain continues to function.
            The phrase "Use it or lose it" is frequently used to describe the environmental influence of the learning process on synaptic  pruning,
            however there is little scientific consensus on what exactly is lost. <p> 
            
            <p> What happens when we ask <em>what</em> is lost when we prune a deep neural network? <p> 
        
        <p> Pruning deep neural networks is a very popular technique for model compression. The ``excess capacity'' of a deep neural network is removed by identifying non-essential 
          weights to prune by setting the value of these weights to zero. 
          Pruned models are heavily used in resource constrained environments because reducing the number of weights lowers energy consumption, memory footprint and latency. 
          These resource constrained environments often entail 

          The earliest deep neural network pruning methods, with names such as <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf">Optimal Brain Damage</a>, were heavily
            motivated by neuron pruning -- the premise that one could remove uneccessary weights while retaining overall accuracy. 
          There are now many different pruning methods to choose from [1,2,3,4,5,6,7,8,9], and this research effort has demonstrated a remarkable ability to sparsify a model
            to a fraction of the original weights while giving up minimal test-set accuracy. Relative
              comparisons of different pruning algorithm have largely centered on top-line metrics such as top-1 or top-5 test-set
              accuracy averaged across classes. However, such aggregate measures can hide details in model performance. Is relative performance unaltered 
          by pruning or are some classes and images impacted more than others?<p>
      <div class="descriptions">
      <h3>What does a deep neural network "forget" as it is pruned?</h3>
      <div class="description">
        <h4>Class Level Pruning Impact</h4>
        
        <figure id="fv-vs-attribution" class="grid" style="">
  <div>
    <div class="images">
      <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F51_vase_vase_pot.png?v=1572924624884">
      <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F365_christmas_stocking_sock_christmas_stocking.png?v=1572924132688">
    </div>
    <figcaption>
      <strong>Feature visualization</strong> answers questions about what a network — or parts of a network — are looking for by generating examples.
    </figcaption>
  </div>
  <div>
    <div class="images">
      <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F365_castle_castle_monastery.png?v=1572924345160"/>


      <img src="images/attribution-2.jpg" style="">
    </div>
    <figcaption>
    <strong>Attribution</strong>
      <d-footnote>
        As a young field, neural network interpretability does not yet have standardized terminology.
        Attribution has gone under many different names in the literature — including “feature visualization”! — but recent work seems to prefer terms like “attribution” and “saliency maps”.
      </d-footnote>
    studies what part of an example is responsible for the network activating a particular way.
    </figcaption>
  </div>
</figure>
        
        <h4>Impact on Individual Images</h4>
       <p>  <p>
        
      <strong>bold</strong>, <em>italics</em>,

        
      <div class="description">
        <p> Pruning has a non-uniform impact across classes; a fraction of classes are 
          disproportionately and systematically impacted by the introduction of sparsity.
          in a statistically significant way.
The examples most impacted by pruning, which we term Pruning Identified Exemplars (PIEs),
              are more challenging for both sparse and non-sparse
         suggests that these hard-to-generalize-to images tend to be of lower image quality, mislabelled, entail abstract representations, require fine-grained classification or depict atypical class examples. We conducted a limited human study to inspect a random sample of $1200$ PIE and non-PIE ImageNet images.  We broadly group the properties we codify as indicative of 1) the exemplar being challenging or 2) the task being ill-specified. We introduce these groupings below (after each bucket we report the percentage of PIEs and non-PIEs in each category 
          as a fraction of total PIEs and non-PIE codified):</p>
        <p> 
        </p>
        <p style="margin-top: 2em">
          <div class="imgs-container">
            <div class="img-container">christmas_stocking</div>
            <div class="img-container">castle</div>
            <div class="img-container">SmoothGrad</div>
            <div class="img-container">SmoothGrad</div>
            <div class="img-container">SmoothGrad</div>
          </div>
          <div class="imgs-container">
            <div class="img-container">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F365_christmas_stocking_sock_christmas_stocking.png?v=1572924132688"/>
            </div>
            <div class="img-container">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F365_castle_castle_monastery.png?v=1572924345160"/>
            </div>
            <div class="img-container">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F250_convertible_convertible_sports_car.png?v=1572924365594">
            </div>
            <div class="img-container">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F278_bathtub_tub_bathtub.png?v=1572924394404">
            </div>
            <div class="img-container">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2F51_vase_vase_pot.png?v=1572924624884">
            </div>
          </div>
        </p>
      </div>
      <div class="description" style="margin-top: 3em">
        <h3>How does pruning impact robustness such as sensitivity to image corruptions
          (blur, noise, contrast) and adversarial examples?</h3>
        <p>
        </p>
        <p>
          Below, we show vanilla gradients, <a href="">Integrated gradients</a>, and <a href="">Guided Backpropagation</a> as they apply to 200 randomly chosen images from the <a href="http://www.image-net.org/">ImageNet dataset</a> using the <a href="https://arxiv.org/abs/1512.00567">Inception V3 model</a>.
        </p>
        <p style="margin-bottom: 2em">
          When the model makes a mistake, we show the row with a light
          red background and give the option to choose which mask to visualize -
          label or the prediction. Often times you can see why the model made a mistake!
    </p>
        </div>
</body>
</html>
  
  <d-appendix>
  
   <h3 id="acknowledgements">Acknowledgments</h3>
  <p>We thank the generosity of our peers for valuable input on earlier versions of this work. In particular, we would like to acknowledge the input of Jonas Kemp, Simon Kornblith, Julius Adebayo, Hugo Larochelle, Dumitru Erhan, Nicolas Papernot, Catherine Olsson, Cliff Young, Martin Wattenberg, Utku Evci, James Wexler, Trevor Gale,  Melissa Fabros, Prajit Ramachandran, Pieter Kindermans, Erich Elsen and Moustapha Cisse. We thank the institutional support and encouragement of Dan Nanas, Rita Ruiz, Sally Jesmonth and Alexander Popper.

  
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list distill-prerendered="true"><style>
d-citation-list {
  contain: style;
}

d-citation-list .references {
  grid-column: text;
}

d-citation-list .references .title {
  font-weight: 500;
}
</style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="szegedy2015going"><span class="title">Going deeper with convolutions</span>   <a href="https://arxiv.org/pdf/1409.4842.pdf">[PDF]</a><br>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A., 2015. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1--9. </li><li id="deng2009imagenet"><span class="title">Imagenet: A large-scale hierarchical image database</span>   <a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf">[PDF]</a><br>Deng, J., Dong, W., Socher, R., Li, L., Li, K. and Fei-Fei, L., 2009. Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248--255.  <a href="https://doi.org/10.1109/cvprw.2009.5206848" style="text-decoration:inherit;">DOI: 10.1109/cvprw.2009.5206848</a></li><li id="erhan2009visualizing"><span class="title">Visualizing higher-layer features of a deep network</span>   <a href="https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf">[PDF]</a><br>Erhan, D., Bengio, Y., Courville, A. and Vincent, P., 2009. University of Montreal, Vol 1341, pp. 3. </li><li id="mordvintsev2015inceptionism"><span class="title">Inceptionism: Going deeper into neural networks</span>   <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">[HTML]</a><br>Mordvintsev, A., Olah, C. and Tyka, M., 2015. Google Research Blog. </li><li id="simonyan2013deep"><span class="title">Deep inside convolutional networks: Visualising image classification models and saliency maps</span>   <a href="https://arxiv.org/pdf/1312.6034.pdf">[PDF]</a><br>Simonyan, K., Vedaldi, A. and Zisserman, A., 2013. arXiv preprint arXiv:1312.6034. </li><li id="gatys2015neural"><span class="title">A neural algorithm of artistic style</span>   <a href="https://arxiv.org/pdf/1508.06576.pdf">[PDF]</a><br>Gatys, L.A., Ecker, A.S. and Bethge, M., 2015. arXiv preprint arXiv:1508.06576. </li><li id="mahendran2015understanding"><span class="title">Understanding deep image representations by inverting them</span>   <a href="https://arxiv.org/pdf/1412.0035v1.pdf">[PDF]</a><br>Mahendran, A. and Vedaldi, A., 2015. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5188--5196.  <a href="https://doi.org/10.1109/cvpr.2015.7299155" style="text-decoration:inherit;">DOI: 10.1109/cvpr.2015.7299155</a></li><li id="mneuron2015"><span class="title">Understanding Intra-Class Knowledge Inside {CNN}</span>   <a href="http://arxiv.org/pdf/1507.02379.pdf">[PDF]</a><br>Wei, D., Zhou, B., Torralba, A. and Freeman, W.T., 2015. CoRR, Vol abs/1507.02379. </li><li id="nguyen2016multifaceted"><span class="title">Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</span>   <a href="https://arxiv.org/pdf/1602.03616.pdf">[PDF]</a><br>Nguyen, A., Yosinski, J. and Clune, J., 2016. arXiv preprint arXiv:1602.03616. </li><li id="nguyen2016plug"><span class="title">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</span>   <a href="https://arxiv.org/pdf/1612.00005.pdf">[PDF]</a><br>Nguyen, A., Yosinski, J., Bengio, Y., Dosovitskiy, A. and Clune, J., 2016. arXiv preprint arXiv:1612.00005. </li><li id="szegedy2013intriguing"><span class="title">Intriguing properties of neural networks</span>   <a href="https://arxiv.org/pdf/1312.6199.pdf">[PDF]</a><br>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. arXiv preprint arXiv:1312.6199. </li><li id="netdissect2017"><span class="title">Network Dissection: Quantifying Interpretability of Deep Visual Representations</span>   <a href="https://arxiv.org/pdf/1704.05796.pdf">[PDF]</a><br>Bau, D., Zhou, B., Khosla, A., Oliva, A. and Torralba, A., 2017. Computer Vision and Pattern Recognition. </li><li id="odena2016deconvolution"><span class="title">Deconvolution and checkerboard artifacts</span>   <a href="http://distill.pub/2016/deconv-checkerboard/">[link]</a><br>Odena, A., Dumoulin, V. and Olah, C., 2016. Distill, Vol 1(10), pp. e3.  <a href="https://doi.org/distill.00003" style="text-decoration:inherit;">DOI: distill.00003</a></li><li id="nguyen2015deep"><span class="title">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</span>   <a href="https://arxiv.org/pdf/1412.1897.pdf">[PDF]</a><br>Nguyen, A., Yosinski, J. and Clune, J., 2015. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427--436.  <a href="https://doi.org/10.1109/cvpr.2015.7298640" style="text-decoration:inherit;">DOI: 10.1109/cvpr.2015.7298640</a></li><li id="oygard2015vis"><span class="title">Visualizing GoogLeNet Classes</span>   <a href="https://www.auduno.com/2015/07/29/visualizing-googlenet-classes/">[link]</a><br>Øygard, A., 2015. </li><li id="tyka2016bilateral"><span class="title">Class visualization with bilateral filters</span>   <a href="https://mtyka.github.io/deepdream/2016/02/05/bilateral-class-vis.html">[HTML]</a><br>Tyka, M., 2016. </li><li id="mordvintsev2016deepdreaming"><span class="title">DeepDreaming with TensorFlow</span>   <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb">[link]</a><br>Mordvintsev, A., 2016. </li><li id="nguyen2016synthesizing"><span class="title">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</span>   <a href="https://arxiv.org/pdf/1605.09304.pdf">[PDF]</a><br>Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T. and Clune, J., 2016. Advances in Neural Information Processing Systems, pp. 3387--3395. </li><li id="abadi2016tensorflow"><span class="title">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</span>   <a href="https://arxiv.org/pdf/1603.04467.pdf">[PDF]</a><br>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I.J., Harp, A., Irving, G., Isard, M., Jia, Y., J{\'{o}}zefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man{\'{e}}, D., Monga, R., Moore, S., Murray, D.G., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.A., Vanhoucke, V., Vasudevan, V., Vi{\'{e}}gas, F.B., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y. and Zheng, X., 2016. arXiv preprint arXiv:1603.04467. </li></ol></d-citation-list>
<distill-appendix>
<style>
  distill-appendix {
    contain: layout style;
  }

  distill-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  distill-appendix > * {
    grid-column: text;
  }
</style>
    <h3 id="citation">Citation</h3>
    <p>This work may be cited as:</p>
    <pre class="citation short">Hooker, et al., "Selective Brain Damage: Measuring the Disparate Impact of Model Pruning", 2019.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@article{olah2017feature,
  author = {Hooker, Sara and Courville, Aaron and Dauphin, Yann and Frome, Andrea},
  title = {},
  journal = {},
  year = {},
  note = {},
  doi = {}
}</pre>
    </distill-appendix></d-appendix>
<style>

:host {
  color: rgba(255, 255, 255, 0.5);
  font-weight: 300;
  padding: 2rem 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  background-color: hsl(180, 5%, 15%); /*hsl(200, 60%, 15%);*/
  text-align: left;
  contain: content;
}

.footer-container .logo svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}

.footer-container .logo svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}

.footer-container .logo {
  font-size: 17px;
  font-weight: 200;
  color: rgba(255, 255, 255, 0.8);
  text-decoration: none;
  margin-right: 6px;
}

.footer-container {
  grid-column: text;
}

.footer-container .nav {
  font-size: 0.9em;
  margin-top: 1.5em;
}

.footer-container .nav a {
  color: rgba(255, 255, 255, 0.8);
  margin-right: 6px;
  text-decoration: none;
}
    </script>
</body>
</html>
