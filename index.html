<html>
<head>
  <title>PIE: Pruning Identified Exemplars</title>
  <meta property="og:type" content="article"/>
  <meta property="og:title" content="PIE: Pruning Identified Exemplars"/>
  <meta property="og:description" content="Measuring the Disparate Impact of Model Pruning">
  <meta property="og:url" content="http://pair-code.github.io/"/> 
  <meta property="og:image" content="http://pair-code.github.io/preview.png"/>
  <meta property="og:locale" content="en_US">
  <meta property="og:site_name" content="Google Research">
  <meta name="twitter:card" value="summary_large_image">
  <meta name="twitter:title" content="PIE: Pruning Identified Exemplars">
  <meta name="twitter:description" content="Measuring the disparate impact of model pruning on classes and individual images.">
  <meta name="twitter:url" content="http://pair-code.github.io/">
  <meta name="twitter:image" content="http://pair-code.github.io/images/preview.png">
  <meta name="twitter:image:width" content="560">
  <meta name="twitter:image:height" content="295">
  
  <!--  https://schema.org/Article -->
  <meta property="description" itemprop="description" content="Measuring the disparate impact of model pruning on classes and individual images.">

  <meta property="article:author" content="Sara Hooker">
  <meta property="article:author" content="Yann Dauphine">
  <meta property="article:author" content="Aaron Courville">
  <meta property="article:author" content="Andrea Frome">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
  <script src="template.v1.js"></script>

  <style>
     body {
      font-family: "Roboto", "Helvetica", sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      font-size: 12px;
    }

    html {
      margin: 0;
      padding: 0;
      height: 100%;
    }

    table td {
      font-size: 12px;
      text-align: center;
      outline: 1px solid white;
      padding: 0;
      margin: 0;
    }

    table.inner td {
      padding: 0;
      margin: 0;
      border: 0;
      width: 25%;
    }

    .footer-row {
      height: 15px;
    }

    .incorrect-row {
      background-color: #ffe8e8;
    }

    .incorrect-row td {
      outline: 1px solid #ffe8e8;
    }

    table.inner tr {
      border: 0;
    }

    table.inner th {
      padding: 8px;
    }

    table th {
      font-size: 11px;
    }

    table {
      border-collapse: collapse;
      border-spacing: 0;
    }

    thead, tbody { display: block; }

    .rotated {
      transform: rotate(90deg);
      transform-origin: left bottom 0;
      margin-top: -111px;
      font-weight: bold;
      font-size: 1.2em;
      padding: 8px;
    }

    #headers {
      z-index: 1000;
      background-color: white;
      height: 65px;
      vertical-align: middle;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }

    #headers span {
      background-color: white;
      display: inline-block;
      line-height: 65px;
      font-size: 1.2em;
      font-weight: bold;
      text-align: center;
      text-overflow: ellipsis;
      white-space: nowrap;
    }

    .cover {
      background: #1e283a;
    }

    .cover-container {
      padding-top: 10px;
      padding-bottom: 60px;
    }
    .descriptions {
      padding-top: 20px;
    }
    .cover-container, .descriptions {
      padding-right: 15px;
      padding-left: 15px;
      margin-right: auto;
      margin-left: auto;   
    }
    
  
    @media (min-width: 415px) {
      authors .authors-affiliations,
       .base-grid,
      .cover-container, .descriptions {
        width: 500px;
      }
    }

    @media (min-width: 768px) {
      authors .authors-affiliations,
      .cover-container, .descriptions {
        width: 650px;
      }
    }

    @media (min-width: 992px) {
      authors .authors-affiliations,
      .cover-container, .descriptions {
        width: 770px;
      }
    }

    @media (min-width: 1200px) {
      authors .authors-affiliations,
      .cover-container, .descriptions {
        width: 970px;
      }
    }

    .cover h1 {
      font-family: "Roboto", "Gotham A", "Gotham B";
      letter-spacing: 0.05em;
      font-size: 63px;
      font-weight: 700;
      margin-bottom: 0.5em;
      text-transform: uppercase;
    }

    .cover h3 {
      font-size: 30px;
      letter-spacing: 0.05em;
      font-weight: 500;
    }

    .descriptions h3 {
      color: #313b4e;
      opacity: .8;
    }

    .cover {
      color: #ddd;
    }
    
    .authors {
      margin-top: -40px;
      overflow: hidden;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    font-size: 1.5rem;
    line-height: 1.8em;
    padding: 1.5rem 0;
    min-height: 1.8em;
    }
    

    .subtitle {
      margin-top: -20px;
    }

    .icons {
      margin-top: 30px;
      padding-left: 4px;
    }

    .icons a {
      display: inline-block;
      font-size: 16px;
      color: #ccc;
      text-decoration: none;
    }

    .paper-icon {
      display: inline-block;
    }

    .paper-icon a {
      line-height: 35px;
      vertical-align: top;
    }

    .paper-icon:hover a {
      cursor: pointer;
      text-decoration: underline;
    }

    .description p {
      width: 75%;
      font-size: 16px;
    }

    .description img {
      vertical-align: middle;
      width: 100%;
    }

    .imgs-container {
      display: table-row;
    }

    .img-container {
      color: #62779c;
      text-align: center;
      font-weight: bold;
      font-size: 14px;
      padding-right: 10px;
      display: table-cell;
      width: 33%;
    }

    #headers.fixed-header {
      position: fixed;
      top: 0;
    }

    #table-container.fixed-header {
      margin-top: 106px;
    }

    .image-label {
      font-size: 15px;
      text-align: left;
      padding-bottom: 4px;
      padding-top: 6px;
      padding-left: 2px;
      font-weight: normal;
    }

    .img-times-selector-container {
      margin-left: -80px;
      margin-top: -45px;
      font-size: 18px;
      font-weight: bold;
      text-align: center;
     }

    .img-times-selector {
      width: 175px;
    }

    #table {
      margin-top: 4px;
      width: 100%;
    }
    
  </style>
</head>
<body>
  <div id="scroll-container">
    <div class="cover">
      <div class="cover-container">
        <div class="icons">
          <div class="paper-icon">
            <a href="https://arxiv.org/abs/1706.03825">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fpaper_icon.png?v=1572561063939" style="width: 100px"/><br>Paper
            </a>
          </div>
          <div class="paper-icon" style="margin-left: 20px">
            <a href="https://github.com/google-research/google-research/tree/master/pruning_identified_exemplars">
              <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fcode_icon.png?v=1572562103868" style="width: 100px"/><br>Code
            </a>
          </div>    
        </div>
        <div class="title"><h2>Selective Brain Damage: Measuring the Disparate Impact of Model Compression</h2></div>
        <div class="authors">Sara Hooker, Aaron Courville, Yann Dauphine, Andrea Frome</div>
      <div class="institutions"></div>
       </div>
    </div>
      <div class="descriptions">
       <p> add note here saying this template was made available by James Wexler <p> 

         <h3>What is lost when we prune deep neural networks?</h3>
          <p>Between infancy and adulthood, the number of synapses in our brain first multiply and then fall. 
            Synaptic pruning improves efficiency by removing redundant neurons and strengthening synaptic connections 
            that are most useful for the environment.  
            Despite losing 50 % of all synapses between age two and ten, the brain continues to function.
            The phrase "use it or lose it" is frequently used to describe the environmental influence of the learning process on synaptic  pruning,
            however there is little scientific consensus on <em>what</em> exactly is lost. <p> 
            
            <p>In 1990, a popular paper was published titled Optimal Brain Damage. The paper was amongst the first [1,2,3] to propose that deep neural networks could be pruned of ``excess capacity'' in a similar way 
              to our biological synaptic pruning. The pruning method identifies non-essential weights to remove from the network by setting to zero.
              Today there are many possible pruning methods to chose from, and pruned models likely drive many of the algorithms on your phone [1,2,3].
              At face value, pruning does appear to promise you can can (almost) have it all. State of art pruning methods remove the majority of the weights with a negligable loss to top-1 accuracy. 
              These newly slimmed down networks require less memory, energy consumption and are faster at producing predictions. All these attributes make pruned models ideal for deploying deep neural networks to mobile phones and other edge devices.
          
            <p> However, it is equally unclear <em>what</em> exactly we give up when we prune a deep neural network. The cost to top-1 accuracy appears minimal if it is spread uniformally across all classes, but what if the cost is concentrated in a few classes? What makes performance on certain classes and images more or
              less ``forgettable'' as sparsity is introduced?
        
          <p>Our results are surprising and suggest caution is needed before deploying pruned models to sensitive domains. Our central finding is that pruning systematically impacts certain classes and images more than others. The images most impacted by pruning is a subset of examples that are more challenging for both a dense and sparse model to classify. <p>
      <div class="description">  
      <div class="imgs-container">
          </div>
          <div class="imgs-container">
            <div class="img-container">
            </div>
            <div class="img-container">
            </div>
            <div class="img-container">
            </div>
          </div>
          <div class="img-container">1. Pruning has a non-uniform impact across classes; a fraction of classes are disproportionately and systematically impacted by the introduction of sparsity.</div>
          <div class="img-container">2. The examples most impacted by pruning, which we term Pruning Identified Exemplars (PIEs), are more challenging for both sparse and non-sparse models to classify.</div>
          <div class="img-container">3. Pruning significantly reduces robustness to image corruptions and natural adversarial images.</div>
      <div class="descriptions">
      <h3>What does a deep neural network "forget" as it is pruned?</h3>
      <div class="description">
        <h4>Class Level Pruning Impact</h4>
        
        <p> If the impact of pruning was uniform, we would expect each class accuracy to shift by the same 
          number of percentage points as the difference in top-1 accuracy between the sparse and non-sparse model.<p> 
        <p>   This forms our <strong>null hypothesis</strong> --  the shift in accuracy for each class 
          before and after pruning is the same as the shift in top-1 accuracy. <p> 
        <p> For each class we consider whether
          to reject the null hypothesis and accept the <strong>alternate hypothesis</strong>
          that pruning disparately affected the class's accuracy in either a positive or negative direction.  <p> 
         <p>  We use a two-tailed independent Welch's t-test to determine whether there is a statistically significant deviation
          from the expected shift for each class in the dataset. More details about our methodology can be found in <p> 
        
        <p> The directionality and magnitude of the impact is nuanced and surprising.
          Our results show that certain classes are relatively robust to the overall degradation 
          experienced by the model whereas others degrade in performance far more than the model itself. 
          This amounts to selective <em>``brain damage"</em> with performance on certain classes evidencing
          far more sensitivity to the removal of model capacity. <p> 
          <p> More classes show a significant relative increase in accuracy than a decrease at every level, 
          though the overall model accuracy decreases at every pruning level, indicating that the magnitude
          of class decreases must be larger in order to pull the model accuracy lower.<p>
        
<div class="gallery">
  <a target="_blank" href="">
    <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fdownload_50.png?v=1573258995667" alt="Cinque Terre" width="600" height="400">
  </a>
  <div class="desc">Add a description of the image here</div>
</div>

<div class="gallery">
  <a target="_blank" href="img_forest.jpg">
    <img src="img_forest.jpg" alt="Forest" width="600" height="400">
  </a>
  <div class="desc">Add a description of the image here</div>
</div>

<div class="gallery">
  <a target="_blank" href="img_lights.jpg">
    <img src="img_lights.jpg" alt="Northern Lights" width="600" height="400">
  </a>
  <div class="desc">Add a description of the image here</div>
</div>

<div class="gallery">
  <a target="_blank" href="img_mountains.jpg">
    <img src="img_mountains.jpg" alt="Mountains" width="600" height="400">
  </a>
  <div class="desc">Add a description of the image here</div>
</div>

        
    <svg width="300" height="500"></svg>

    <script src="https://button.glitch.me/button.js"></script>
        
        <figure id="fv-vs-attribution" class="grid" style="">
  <div>
    <div class="images">
      <img src="">
      <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fdownload_50.png?v=1573258995667">
    </div>
    <figcaption>
      <strong>Feature visualization</strong> answers questions about what a network — or parts of a network — are looking for by generating examples.
    </figcaption>
  </div>
  <div>
    <div class="images">
      <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fdownload_50.png?v=1573258995667"/>
   <img src="https://cdn.glitch.com/a08d19a0-dea5-4f06-9627-caa859e2d931%2Fdownload_50.png?v=1573258995667"/>
    </div>
    <figcaption>
    <strong>Attribution</strong>   
        <figcaption>

</figure>
        <h4>Impact on Individual Images</h4>
       <p>  <p>
        
      <strong>bold</strong>, <em>italics</em>,

        
      <div class="description">
        <p> Pruning has a non-uniform impact across classes; a fraction of classes are 
          disproportionately and systematically impacted by the introduction of sparsity.
          in a statistically significant way.
The examples most impacted by pruning, which we term Pruning Identified Exemplars (PIEs),
              are more challenging for both sparse and non-sparse
         suggests that these hard-to-generalize-to images tend to be of lower image quality, mislabelled, entail abstract representations, require fine-grained classification or depict atypical class examples. We conducted a limited human study to inspect a random sample of $1200$ PIE and non-PIE ImageNet images.  We broadly group the properties we codify as indicative of 1) the exemplar being challenging or 2) the task being ill-specified. We introduce these groupings below (after each bucket we report the percentage of PIEs and non-PIEs in each category 
          as a fraction of total PIEs and non-PIE codified):</p>
        </div>
  
      
<dt-appendix>
<h3 id="acknowledgements">Acknowledgments</h3>
 <p>We thank the generosity of our peers for valuable input on earlier versions of this work. In particular, we would like to acknowledge the input of Jonas Kemp, Simon Kornblith, Julius Adebayo, Hugo Larochelle, Dumitru Erhan, Nicolas Papernot, Catherine Olsson, Cliff Young, Martin Wattenberg, Utku Evci, James Wexler, Trevor Gale,  Melissa Fabros, Prajit Ramachandran, Pieter Kindermans, Erich Elsen and Moustapha Cisse. We thank the institutional support and encouragement of Dan Nanas, Rita Ruiz, Sally Jesmonth and Alexander Popper.
This article was prepared partly relying on the Distill template.</p>
  <h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">Hooker et al., "Selective Brain Damage: Measuring the Disparate Impact of Model Pruning", 2019.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{Ha2018designrl,
  author = {Hooker, Sara and Courville, Aaron and Dauphin, Yann and Frome, Andrea},
  title  = {Selective Brain Damage: Measuring the Disparate Impact of Model Pruning},
  eprint = {arXiv:1810.03779},
  url    = {https://designrl.github.io},
  note   = "\url{https://designrl.github.io}",
  year   = {2019}
}</pre>
  

</body>
</html>
